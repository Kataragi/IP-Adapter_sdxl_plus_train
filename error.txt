encoder_hidden_states.shape: torch.Size([2, 77, 768])
image_embeds.shape: torch.Size([2, 257, 1280])
encoder_hidden_states after expansion: torch.Size([2, 77, 1280])
ip_tokens.shape before processing: torch.Size([2, 257, 1280])
encoder_hidden_states after concatenation: torch.Size([2, 334, 1280])
Training Progress:   0%|                                                                   | 0/100000 [00:01<?, ?step/s]
Traceback (most recent call last):
  File "/home/akishiro/IP-Adapter/kataragi-IP-adapter_sdxl_plus.py", line 619, in <module>
    main()
    ^^^^^^
  File "/home/akishiro/IP-Adapter/kataragi-IP-adapter_sdxl_plus.py", line 586, in main
    noise_pred = ip_adapter(noisy_latents, timesteps, encoder_hidden_states, unet_added_cond_kwargs, image_embeds)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/accelerate/utils/operations.py", line 823, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/accelerate/utils/operations.py", line 811, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/kataragi-IP-adapter_sdxl_plus.py", line 231, in forward
    noise_pred = self.unet(
                 ^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/diffusers/models/unets/unet_2d_condition.py", line 1216, in forward
    sample, res_samples = downsample_block(
                          ^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/diffusers/models/unets/unet_2d_blocks.py", line 1288, in forward
    hidden_states = attn(
                    ^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/diffusers/models/transformers/transformer_2d.py", line 442, in forward
    hidden_states = block(
                    ^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/diffusers/models/attention.py", line 545, in forward
    attn_output = self.attn2(
                  ^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/diffusers/models/attention_processor.py", line 495, in forward
    return self.processor(
           ^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/ip_adapter/attention_processor.py", line 358, in __call__
    key = attn.to_k(encoder_hidden_states)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected size for first two dimensions of batch2 tensor to be: [2, 1280] but got: [2, 2048].
Traceback (most recent call last):
  File "/home/akishiro/IP-Adapter/venv/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/accelerate/commands/launch.py", line 1168, in launch_command
    simple_launcher(args)
  File "/home/akishiro/IP-Adapter/venv/lib/python3.12/site-packages/accelerate/commands/launch.py", line 763, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/akishiro/IP-Adapter/venv/bin/python3', 'kataragi-IP-adapter_sdxl_plus.py', '--pretrained_model_name_or_path=/home/akishiro/IP-Adapter/models/animagine-xl-3.1', '--image_encoder_path=kataragi/Image_encoder_VitH', '--pretrained_ip_adapter_path=/home/akishiro/IP-Adapter/models/pretrained_ip_adapter/ip-adapter-plus_sdxl_vit-h.bin', '--data_json_file=/home/akishiro/IP-Adapter/data.json', '--data_root_path=/home/akishiro/IP-Adapter/dataset', '--learning_rate=1e-04', '--resolution=1024', '--train_batch_size=2', '--weight_decay=0.05', '--num_train_epochs=1', '--output_dir=/home/akishiro/IP-Adapter/output_model', '--save_steps=50', '--num_tokens=16']' returned non-zero exit status 1.
